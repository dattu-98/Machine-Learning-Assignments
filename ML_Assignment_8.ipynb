{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06fd94d4",
   "metadata": {},
   "source": [
    "**1. What exactly is a feature? Give an example to illustrate your point.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811b779e",
   "metadata": {},
   "source": [
    "Features are nothing but the input classes of a data set. Its not mandatory to select all the features in the dataset. Only the features which are suitable that solves the problem should be selected. There are various feature selection methods. So,the features are the descriptive attributes, and the label is what you're attempting to predict or forecast. \n",
    "\n",
    "With the help of methods like Filter Methods, Wrapper methods, Embedded methods, hybrid methods we can select the features. The techniques involved are Information gain, Chi-Square test, Fishers score, Correlation coefficient, Variance thershold, MAD, Forward feature selection, backward feature selection etc...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a0f452",
   "metadata": {},
   "source": [
    "**2. What are the various circumstances in which feature construction is required?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c658e7",
   "metadata": {},
   "source": [
    "Feature construction involves transforming a given set of input features to generate a new set of more powerful features which can then used for prediction. Mainly this is done to improve the performance of the model. To develop or create new feature, already existing features are used. This also reduces the code and simplicity by constructing the required feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ebfbbd",
   "metadata": {},
   "source": [
    "**3. Describe how nominal variables are encoded.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5468cf3",
   "metadata": {},
   "source": [
    "If a column with nominal variables has values that cannot be ordered in any meaningful way, then it is most way to do is one-hot (aka dummy) encoding, but there are many options that might perform better for machine learning.  In one hot encoding, for each level of a categorical feature, we create a new variable. Each category is mapped with a binary variable containing either 0 or 1. Here, 0 represents the absence, and 1 represents the presence of that category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f9449b",
   "metadata": {},
   "source": [
    "**4. Describe how numeric features are converted to categorical features.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f06b3a5e",
   "metadata": {},
   "source": [
    "We convert numerical data to categorical features using Bucketing or Binning. Again Bucketing is of two types Bucketing with Equally spaced boundaries, Bucketing with Quantile boundaries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ef221",
   "metadata": {},
   "source": [
    "**5. Describe the feature selection wrapper approach. State the advantages and disadvantages of this approach?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36649c8",
   "metadata": {},
   "source": [
    "Wrapper methods evaluates on a specific machine learning algorithm to find the optimal features. It follows a greedy search approach by evaluating all the possible combinations of features against the evaluation criterion. The evaluation criterion is simply the performance measure which depends on the type of problem. \n",
    "\n",
    "There may be chances of over fitting of data, because it involves training of ML models with different combination of features. Examples are: Forward Selection, Stepwise selection, Backward elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2dba02",
   "metadata": {},
   "source": [
    "**6. When is a feature considered irrelevant? What can be said to quantify it?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec622e92",
   "metadata": {},
   "source": [
    "Irrelevant features may make the learning algorithms less efficient. Highly correlated features also contain same information. So its not necessary to give all the features to the model. Noisy features also reduces the efficiency of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0edc3e9",
   "metadata": {},
   "source": [
    "**7. When is a function considered redundant? What criteria are used to identify features that could be redundant?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f56fb1e",
   "metadata": {},
   "source": [
    "Redundant features slows down the training process, This will slow down the calculations of the model. When there is high correlation in the features of a data set then it is said to be redundant. Correlation is foud out. Coefficients like Pearson’s Correlation coefficient, Spearman’s Correlation coefficient is found out. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b569fd",
   "metadata": {},
   "source": [
    "**8. What are the various distance measurements used to determine feature similarity?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02df12fa",
   "metadata": {},
   "source": [
    "There are various distance measurements like Euclidean distance, Manhattan distance, Minkowski distance, Cosine Similarity, Jackard Similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78b3f15",
   "metadata": {},
   "source": [
    "**9. State difference between Euclidean and Manhattan distances?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec74247",
   "metadata": {},
   "source": [
    "Euclidean Distance: Euclidean distance between two points in Euclidean space is the length of a line segment between the two points. The Pythagorean theorem gives this distance between two points.\n",
    "\n",
    "![](https://www.tutorialexample.com/wp-content/uploads/2020/05/Euclidean-distance-in-tensorflow.png)\n",
    "\n",
    "Manhattan Distance: Manhattan distance is a metric in which the distance between two points is calculated as the sum of the absolute differences of their Cartesian coordinates.\n",
    "\n",
    "![](https://miro.medium.com/max/814/0*_9ljPf7RbVI5cVdG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384f1df6",
   "metadata": {},
   "source": [
    "**10. Distinguish between feature transformation and feature selection.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d18edb",
   "metadata": {},
   "source": [
    "Feature selection: Feature selection is nothing but selecting the most favourable features that accurately predicts the output. There will be many features in a data set. But only few features play a crucial role in predicting the future value. So there are different techniques for feature selection like Wrapper methods, Embedded methods, Wrapper methods. \n",
    "\n",
    "Feature Transformation: Feature transformation is a mathematical transformation in which we apply a mathematical formula to a particular column / feature and transform the values which are useful for our further analysis. There are three transformation in Sklearn library they are: Function Transformation, Power Transformation , Quantile transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8aacdd",
   "metadata": {},
   "source": [
    "**11. Make brief notes on any two of the following:\n",
    "          1.SVD (singular value decomposition)\n",
    "          2. Collection of features using a hybrid approach\n",
    "          3. The width of the silhouette\n",
    "          4. Receiver operating characteristic curve**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed27b5c2",
   "metadata": {},
   "source": [
    "1. singular value decomposition: SVD is a matrix decomposition method. The Singular-Value Decomposition, or SVD for short, is a matrix decomposition method for reducing a matrix to its constituent parts in order to make certain subsequent matrix calculations simpler. The SVD is used widely both in the calculation of other matrix operations, such as matrix inverse, but also as a data reduction method in machine learning.\n",
    "\n",
    "2. A hybrid feature selection method is proposed for classification in small sample size data sets. The filter step is based on instance learning taking advantage of the small sample size of data. Cooperative feature subset search is proposed with a classifier algorithm for the wrapper step. The proposed method improves classification accuracy and stability of feature selection.\n",
    "\n",
    "3. The silhouette analysis measures how well an observation is clustered and it estimates the average distance between clusters. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters. \n",
    "\n",
    "4. ROC Curve: ROC is a probability curve i.e, Receiver operating characteristic curve. This curve plots two parameters: True Positive Rate, False Positive Rate. \n",
    "\n",
    "![](https://miro.medium.com/max/722/1*pk05QGzoWhCgRiiFbz-oKQ.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
